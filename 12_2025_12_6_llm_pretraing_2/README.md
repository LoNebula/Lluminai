# Modern Transformer Implementation (Llama Style) ğŸ¦™

GPT-2ãªã©ã®å¾“æ¥ã®Transformerã§ã¯ãªãã€**Llama 2/3 ã‚„ Mistral ãªã©ã®SOTAï¼ˆState-of-the-Artï¼‰ãƒ¢ãƒ‡ãƒ«ã§æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹ã€Œãƒ¢ãƒ€ãƒ³ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€** ã‚’PyTorchã§ãƒ•ãƒ«ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã—ãŸãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚

Zennè¨˜äº‹é€£è¼‰ **ã€ŒLLMè‡ªä½œå…¥é–€ã€Vol.2** ã®å®Ÿè¨¼ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦ä½œæˆã€‚

## ğŸš€ Key Features

ç¾ä»£ã®LLMé–‹ç™ºã«ãŠã‘ã‚‹ "ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰" ã¨ãªã‚‹ä»¥ä¸‹ã®3ã¤ã®æŠ€è¡“è¦ç´ ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

1.  **RMSNorm (Root Mean Square Layer Normalization)** âš–ï¸
    * å¾“æ¥ã® `LayerNorm` (Mean & Variance) ã§ã¯ãªãã€äºŒä¹—å¹³å‡ã®ã¿ã§æ­£è¦åŒ–ã€‚
    * è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã¤ã¤ã€æ·±å±¤å­¦ç¿’ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚
2.  **RoPE (Rotary Positional Embeddings)** ğŸŒ€
    * çµ¶å¯¾ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆAbsolute PEï¼‰ã‚’å»ƒæ­¢ã—ã€è¤‡ç´ æ•°æ¼”ç®—ã‚’ç”¨ã„ãŸã€Œå›è»¢ã€ã«ã‚ˆã‚‹ç›¸å¯¾ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’æ¡ç”¨ã€‚
    * å­¦ç¿’æ™‚ã‚ˆã‚Šã‚‚é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã¸ã®å¤–æŒ¿æ€§èƒ½ï¼ˆExtrapolationï¼‰ã‚’ç†è«–çš„ã«ä¿è¨¼ã—ã¾ã™ã€‚
3.  **SwiGLU Activation** ğŸšª
    * `ReLU` ã‚„ `GELU` ã§ã¯ãªãã€Gatingæ©Ÿæ§‹ã‚’æŒã¤ `SiLU` ãƒ™ãƒ¼ã‚¹ã®æ´»æ€§åŒ–é–¢æ•°ã‚’æ¡ç”¨ã€‚
    * ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯å¢—åŠ ã—ã¾ã™ãŒã€Scaling Lawsã«ãŠã‘ã‚‹å­¦ç¿’åŠ¹ç‡ãŒå‘ä¸Šã—ã¾ã™ã€‚

## ğŸ›  Architecture Overview

æœ¬å®Ÿè£…ã¯ **Decoder-only Transformer** ã§ã‚ã‚Šã€ä»¥ä¸‹ã®æ§‹æˆã‚’æŒã¡ã¾ã™ã€‚

* **Pre-Normalization**: å„ãƒ–ãƒ­ãƒƒã‚¯ã®å…¥åŠ›å´ã§æ­£è¦åŒ–ï¼ˆå­¦ç¿’ã®å®‰å®šåŒ–ï¼‰ã€‚
* **No Bias**: Linearå±¤ã‚„Normå±¤ã‹ã‚‰ãƒã‚¤ã‚¢ã‚¹é …ã‚’é™¤å»ï¼ˆLlama/PaLMæµå„€ï¼‰ã€‚
* **Flash Attention Ready**: PyTorch 2.0+ ã® `F.scaled_dot_product_attention` ã‚’ä½¿ç”¨ã€‚

## ğŸ“¦ Requirements

* Python 3.8+
* PyTorch 2.0+ (Required for Flash Attention & Complex Float support)

## ğŸ’» Usage

### 1. Model Configuration
`ModelArgs` ã‚¯ãƒ©ã‚¹ã§ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚’æŸ”è»Ÿã«å®šç¾©ã§ãã¾ã™ã€‚

```python
from model import ModelArgs, Transformer

# Configure the model (e.g., Mini Llama)
args = ModelArgs(
    dim=512,
    n_layers=8,
    n_heads=8,
    vocab_size=32000,
    max_seq_len=512,
    multiple_of=256
)

# Initialize
model = Transformer(args)
print(f"Parameters: {sum(p.numel() for p in model.parameters()):,}")
```

### 2\. Forward Pass (Inference)

ãƒˆãƒ¼ã‚¯ãƒ³IDã®ãƒãƒƒãƒã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€Logitsã‚’å‡ºåŠ›ã—ã¾ã™ã€‚

```python
import torch

# Dummy input (Batch=2, Seq=128)
x = torch.randint(0, args.vocab_size, (2, 128))

# Forward pass
logits = model(x)
print(logits.shape) # torch.Size([2, 128, 32000])
```

## ğŸ“ Code Structure

  * `RMSNorm`: æ­£è¦åŒ–å±¤ã®å®Ÿè£… (`torch.rsqrt` ä½¿ç”¨)
  * `precompute_freqs_cis` & `apply_rotary_emb`: RoPEã®äº‹å‰è¨ˆç®—ã¨é©ç”¨ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆè¤‡ç´ æ•°æ¼”ç®—ï¼‰
  * `FeedForward`: SwiGLUã‚’æ¡ç”¨ã—ãŸFFN (Gate, Up, Down projections)
  * `Attention`: Multi-Head Attention (Flash Attentionå¯¾å¿œ)
  * `TransformerBlock` & `Transformer`: å…¨ä½“ã®çµ„ã¿ç«‹ã¦
