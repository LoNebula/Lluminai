{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLQKq4hsSY8y",
        "outputId": "2652cf4a-e2f7-4b46-a08b-552faaddce62"
      },
      "outputs": [],
      "source": [
        "!pip install datasets tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "6adb3e9147b64fbd83338e63d8e5cc7b",
            "f80f629dabb645318bd6506e636d5300",
            "48f6cb0eeacf4331abd9ca67ae3ff594",
            "9fa7d3cef2bc48d0aee62aead36c4330",
            "7d88b62e539e485bb068fb49522581b2",
            "d19b7f9bb7b34fc9be88b783c6bb5ba8",
            "844adb9ab98d4b418b8ff6fb3ba85c90",
            "5fc84a8f40f34bc2a51fbd2289b3abad",
            "22c3b495c9964fe185a74ec1892a9b28",
            "19e455b1bee741d5951db8eb9f623ccf",
            "5a02848b475f46e5a673949a0d4be068"
          ]
        },
        "id": "fovZli-DOwaR",
        "outputId": "23be101e-1e30-497b-bdf5-871899f4ab06"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import tqdm\n",
        "\n",
        "# 1. データセットのロード（ストリーミングモードでメモリ節約）\n",
        "print(\"データセットに接続中...\")\n",
        "dataset = load_dataset(\"izumi-lab/wikipedia-ja-20230720\", split=\"train\", streaming=True)\n",
        "\n",
        "# 2. 保存処理\n",
        "CORPUS_FILE = \"wiki_ja_subset.txt\"\n",
        "print(f\"ダウンロードと保存を開始します: {CORPUS_FILE}\")\n",
        "\n",
        "with open(CORPUS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    # 最初の1万件だけを取得\n",
        "    for i, data in enumerate(tqdm.tqdm(dataset)):\n",
        "        # 改行を除去して1行にする\n",
        "        text = data[\"text\"].replace(\"\\n\", \"\")\n",
        "\n",
        "        # 短すぎる記事は除外（100文字以上のみ保存）\n",
        "        if len(text) > 100:\n",
        "            f.write(text + \"\\n\")\n",
        "\n",
        "        # 10,000件でストップ\n",
        "        if i >= 10000:\n",
        "            break\n",
        "\n",
        "print(\"✅ データセット作成完了！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNgHuke0OyWD",
        "outputId": "804dc9ae-bb06-46d4-feed-7a4b14a014d3"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "\n",
        "# 1. モデルの初期化（BPE）\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# 2. Pre-tokenizerの設定（バイトレベルで分割）\n",
        "# GPT-2などと同様、スペースも含めて処理する設定\n",
        "tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=False)\n",
        "\n",
        "# 3. Decoderの設定（IDから文字列に戻す用）\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "# 4. Trainerの設定\n",
        "# vocab_size: 語彙数（日本語LLMでは32000〜64000程度が一般的）\n",
        "# min_frequency: 登場回数がこれ以下のサブワードは作らない\n",
        "vocab_size = 32000\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=vocab_size,\n",
        "    min_frequency=2,\n",
        "    special_tokens=[\"<|endoftext|>\", \"<|pad|>\"], # 特殊トークンの定義\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "# 5. 学習実行\n",
        "tokenizer.train([CORPUS_FILE], trainer)\n",
        "\n",
        "# 6. 保存\n",
        "tokenizer.save(\"custom_tokenizer.json\")\n",
        "print(f\"トークナイザ学習完了。Vocab Size: {tokenizer.get_vocab_size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njw06AGNPIVn",
        "outputId": "cb3e6d66-249a-4aa9-d7be-df4f8c111ced"
      },
      "outputs": [],
      "source": [
        "test_sentence = \"生成AIの技術は日進月歩で進化しています。\"\n",
        "encoded = tokenizer.encode(test_sentence)\n",
        "\n",
        "print(f\"Original: {test_sentence}\")\n",
        "print(f\"Tokens:   {encoded.tokens}\")\n",
        "print(f\"IDs:      {encoded.ids}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1eQZD54ViZV",
        "outputId": "86f0f6c0-5855-41fa-e50a-c24c62005622"
      },
      "outputs": [],
      "source": [
        "# 1つずつのIDを元の文字に戻して表示する検証コード\n",
        "print(f\"元の文: {test_sentence}\\n\")\n",
        "print(\"--- トークンごとの分割内訳 ---\")\n",
        "\n",
        "for t_id in encoded.ids:\n",
        "    # IDを1つだけデコードする\n",
        "    word = tokenizer.decode([t_id])\n",
        "    print(f\"ID: {t_id:5d} | トークン: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn_zAZ_lPJ37",
        "outputId": "182c5247-a40a-48d8-e4c9-3a1118b0ce56"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class LLMPretrainDataset(Dataset):\n",
        "    def __init__(self, txt_file, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.input_ids = []\n",
        "\n",
        "        # 全テキストを読み込んでトークナイズ（メモリ注意：本番ではmmap等を使う）\n",
        "        with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # 一気にエンコード\n",
        "        tokens = tokenizer.encode(text).ids\n",
        "\n",
        "        # max_lengthごとに分割（簡易的なスライディングウィンドウなしの実装）\n",
        "        # strideをつける場合は step = max_length (オーバーラップなし)\n",
        "        for i in range(0, len(tokens) - max_length, max_length):\n",
        "            self.input_ids.append(tokens[i : i + max_length])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # input:  x_1, x_2, ..., x_T\n",
        "        # target: x_2, x_3, ..., x_{T+1} (1つ右にずらす)\n",
        "        chunk = self.input_ids[idx]\n",
        "\n",
        "        x = torch.tensor(chunk, dtype=torch.long)\n",
        "        y = torch.tensor(chunk, dtype=torch.long) # 実際はずらしてLoss計算時に処理するが、ここではデータとしては同じものを返すのが通例\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# 動作確認\n",
        "dataset = LLMPretrainDataset(CORPUS_FILE, tokenizer, max_length=128)\n",
        "print(f\"総サンプル数: {len(dataset)}\")\n",
        "\n",
        "x, y = dataset[0]\n",
        "print(f\"Input shape: {x.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqi0KznvhsPD"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 512          # 埋め込み次元 (d_model)\n",
        "    n_layers: int = 8       # レイヤー数\n",
        "    n_heads: int = 8        # Attentionヘッド数\n",
        "    vocab_size: int = 32000 # 語彙数 (Vol.1で作成したtokenizerに合わせる)\n",
        "    multiple_of: int = 256  # SwiGLUの隠れ層次元の調整用\n",
        "    max_seq_len: int = 512  # 最大コンテキスト長\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    # 計算用プロパティ\n",
        "    @property\n",
        "    def head_dim(self):\n",
        "        return self.dim // self.n_heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BlBUtNlhxWR"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        # x: (Batch, Seq, Dim)\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMoKHqW8hyPx"
      },
      "outputs": [],
      "source": [
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    # 回転角度の事前計算 (複素数平面で考えると楽)\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
        "    freqs = torch.outer(t, freqs).float()  # (Seq, Dim/2)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis\n",
        "\n",
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    # xq, xk: (Batch, Seq, Head, HeadDim) -> 複素数化して回転\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "\n",
        "    # broadcastingのためにshapeを合わせる\n",
        "    freqs_cis = freqs_cis[:xq.shape[1]].view(1, xq.shape[1], 1, -1)\n",
        "\n",
        "    # 回転適用\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oZipFiAh2Bq"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.head_dim = args.head_dim\n",
        "\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        # x: (Batch, Seq, Dim)\n",
        "        bsz, seqlen, _ = x.shape\n",
        "\n",
        "        # Q, K, V の射影 & Head分割\n",
        "        xq = self.wq(x).view(bsz, seqlen, self.n_heads, self.head_dim)\n",
        "        xk = self.wk(x).view(bsz, seqlen, self.n_heads, self.head_dim)\n",
        "        xv = self.wv(x).view(bsz, seqlen, self.n_heads, self.head_dim)\n",
        "\n",
        "        # RoPE の適用 (QとKを回転させる)\n",
        "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis)\n",
        "\n",
        "        # Flash Attention (is_causal=True で因果マスク自動適用)\n",
        "        output = F.scaled_dot_product_attention(\n",
        "            xq.transpose(1, 2), # (B, H, S, D)\n",
        "            xk.transpose(1, 2),\n",
        "            xv.transpose(1, 2),\n",
        "            is_causal=True\n",
        "        )\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
        "        return self.wo(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir9dLs3qh34-"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        # 隠れ層のサイズ計算 (Llamaの仕様：2/3 * 4d 程度にしてmultiple_of倍数にする)\n",
        "        hidden_dim = 4 * args.dim\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n",
        "\n",
        "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False) # Gate\n",
        "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False) # Down\n",
        "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False) # Up\n",
        "\n",
        "    def forward(self, x):\n",
        "        # SwiGLU: w2( SiLU(w1(x)) * w3(x) )\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvZ_f24Th58Z"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.attention_norm = RMSNorm(args.dim)\n",
        "        self.attention = Attention(args)\n",
        "        self.ffn_norm = RMSNorm(args.dim)\n",
        "        self.feed_forward = FeedForward(args)\n",
        "\n",
        "    def forward(self, x, freqs_cis):\n",
        "        # Residual Connection (Pre-Norm)\n",
        "        h = x + self.attention(self.attention_norm(x), freqs_cis)\n",
        "        out = h + self.feed_forward(self.ffn_norm(h))\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(args) for _ in range(args.n_layers)])\n",
        "        self.norm = RMSNorm(args.dim) # Final Norm\n",
        "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
        "\n",
        "        # RoPEテーブルの事前計算\n",
        "        self.freqs_cis = precompute_freqs_cis(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # idx: (Batch, Seq)\n",
        "        bsz, seqlen = idx.shape\n",
        "        x = self.tok_embeddings(idx)\n",
        "\n",
        "        # RoPEテーブルをデバイスへ\n",
        "        freqs_cis = self.freqs_cis[:seqlen].to(x.device)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, freqs_cis)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.output(x)\n",
        "        return logits"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "19e455b1bee741d5951db8eb9f623ccf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22c3b495c9964fe185a74ec1892a9b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48f6cb0eeacf4331abd9ca67ae3ff594": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fc84a8f40f34bc2a51fbd2289b3abad",
            "max": 480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22c3b495c9964fe185a74ec1892a9b28",
            "value": 480
          }
        },
        "5a02848b475f46e5a673949a0d4be068": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fc84a8f40f34bc2a51fbd2289b3abad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6adb3e9147b64fbd83338e63d8e5cc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f80f629dabb645318bd6506e636d5300",
              "IPY_MODEL_48f6cb0eeacf4331abd9ca67ae3ff594",
              "IPY_MODEL_9fa7d3cef2bc48d0aee62aead36c4330"
            ],
            "layout": "IPY_MODEL_7d88b62e539e485bb068fb49522581b2"
          }
        },
        "7d88b62e539e485bb068fb49522581b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "844adb9ab98d4b418b8ff6fb3ba85c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fa7d3cef2bc48d0aee62aead36c4330": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19e455b1bee741d5951db8eb9f623ccf",
            "placeholder": "​",
            "style": "IPY_MODEL_5a02848b475f46e5a673949a0d4be068",
            "value": " 480/480 [00:00&lt;00:00, 76.3kB/s]"
          }
        },
        "d19b7f9bb7b34fc9be88b783c6bb5ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f80f629dabb645318bd6506e636d5300": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d19b7f9bb7b34fc9be88b783c6bb5ba8",
            "placeholder": "​",
            "style": "IPY_MODEL_844adb9ab98d4b418b8ff6fb3ba85c90",
            "value": "README.md: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
